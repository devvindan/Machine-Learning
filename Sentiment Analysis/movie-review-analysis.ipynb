{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Movie review sentiment analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given a text of a movie review, predict the sentiment of it. For the following task we are going to use NLTK package and 'movie_reviews' data in particular."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Movie reviews dataset contains positive and negative movie reviews, and might be downloaded using the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\"\\n\\nimport nltk\\nnltk.download(\\'movie_reviews\\')\\n\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\"\n",
    "\n",
    "import nltk\n",
    "nltk.download('movie_reviews')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries for data exprolation\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data import and exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import movie_reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['neg/cv000_29416.txt',\n",
       " 'neg/cv001_19502.txt',\n",
       " 'neg/cv002_17424.txt',\n",
       " 'neg/cv003_12683.txt',\n",
       " 'neg/cv004_12641.txt']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movie_reviews.fileids()[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive reviews: 1000 \n",
      "Negative reviews: 1000\n"
     ]
    }
   ],
   "source": [
    "# selecting negative and positive reviews\n",
    "\n",
    "negids = movie_reviews.fileids('neg')\n",
    "posids = movie_reviews.fileids('pos')\n",
    "print(\"Positive reviews: {} \\nNegative reviews: {}\".format(len(posids), len(negids)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a list of words of positive/negative reviews\n",
    "negfeats = [list(movie_reviews.words(fileids=[f])) for f in negids]\n",
    "posfeats = [list(movie_reviews.words(fileids=[f])) for f in posids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "totalreviews = negfeats + posfeats\n",
    "labels = len(negfeats) * [0] + len(posfeats) * [1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total reviews in dataset: 2000\n"
     ]
    }
   ],
   "source": [
    "print(\"Total reviews in dataset: {}\".format(len(totalreviews)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, our dataset is perfectly balanced. Half of our reviews are positive, and others are negative."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model building"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first try to build a simple model without any preprocessing of review words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform our review representation to strings instead of list of words in order to use in vectorizer\n",
    "totalreviews = [\" \".join(review) for review in totalreviews]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature count: 39659\n"
     ]
    }
   ],
   "source": [
    "vectorizer = CountVectorizer()\n",
    "vectorizer.fit(totalreviews)\n",
    "print(\"Feature count: {}\".format(len(vectorizer.get_feature_names())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our vectorizer creates a matrix *documents X tokens*, where each cell represents token frequency\n",
    "\n",
    "Let's build a pipeline with CountVectorizer and LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_pipeline = Pipeline(steps=[\n",
    "    ('vectorizer', CountVectorizer()),\n",
    "    ('estimator', LogisticRegression())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_scores = cross_val_score(clf_pipeline, totalreviews, labels, scoring='accuracy')\n",
    "roc_auc_scores = cross_val_score(clf_pipeline, totalreviews, labels, scoring='roc_auc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validation accuracy score: 0.8360216503929078\n",
      "Cross-validation roc_auc score: 0.9107764937833774\n"
     ]
    }
   ],
   "source": [
    "print(\"Cross-validation accuracy score: {}\".format(np.mean(accuracy_scores)))\n",
    "print(\"Cross-validation roc_auc score: {}\".format(np.mean(roc_auc_scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model interpretation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look at top-5 most important parameters (words) according to our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 important words according to coefficients:\n",
      "\n",
      "bad\n",
      "unfortunately\n",
      "worst\n",
      "fun\n",
      "waste\n"
     ]
    }
   ],
   "source": [
    "clf_pipeline.fit(totalreviews, labels)\n",
    "print(\"Top 5 important words according to coefficients:\\n\")\n",
    "\n",
    "# look for both negative and positive coefficients\n",
    "\n",
    "indicies = np.argsort(np.abs(clf_pipeline.named_steps['estimator'].coef_[0]))\n",
    "feature_names = clf_pipeline.named_steps['vectorizer'].get_feature_names()\n",
    "\n",
    "for index in reversed(indicies[-5:]):\n",
    "    print(feature_names[index])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see so far, coefficients of our model make sense."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters setting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Insted of Count Vectorizer we can use TF-IDF method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_pipeline = Pipeline(steps=[\n",
    "    ('vectorizer', TfidfVectorizer()),\n",
    "    ('estimator', LogisticRegression())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simple function for estimating pipeline on our data\n",
    "\n",
    "def estimate_pipeline(pipeline, X=totalreviews, y=labels):\n",
    "    scores = cross_val_score(pipeline, X, y, cv=5)\n",
    "    print(\"CV mean score: {} Standard deviation: {}\".format(np.mean(scores), np.std(scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimating pipeline with count vectorizer:\n",
      "CV mean score: 0.8415000000000001 Standard deviation: 0.01677796173556255\n",
      "Estimating pipeline with Tfidf vectorizer:\n",
      "CV mean score: 0.8210000000000001 Standard deviation: 0.004062019202317978\n"
     ]
    }
   ],
   "source": [
    "print(\"Estimating pipeline with count vectorizer:\")\n",
    "estimate_pipeline(clf_pipeline)\n",
    "\n",
    "print(\"Estimating pipeline with Tfidf vectorizer:\")\n",
    "estimate_pipeline(tfidf_pipeline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Changing vectorizer to TFID does not have much effect on performance\n",
    "\n",
    "Let's try to vary cut-off parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "# min_df parameter is responsible for setting a threshhold for ignoring terms with lower frequency\n",
    "\n",
    "df10_pipeline = Pipeline(steps=[\n",
    "    ('vectorizer', CountVectorizer(min_df=10)),\n",
    "    ('estimator', LogisticRegression())\n",
    "])\n",
    "\n",
    "df50_pipeline = Pipeline(steps=[\n",
    "    ('vectorizer', CountVectorizer(min_df=50)),\n",
    "    ('estimator', LogisticRegression())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimating pipeline with count vectorizer and min_df = 10:\n",
      "CV mean score: 0.8390000000000001 Standard deviation: 0.011895377253370336\n",
      "Estimating pipeline with count vectorizer and min_df = 50:\n",
      "CV mean score: 0.813 Standard deviation: 0.013453624047073712\n"
     ]
    }
   ],
   "source": [
    "print(\"Estimating pipeline with count vectorizer and min_df = 10:\")\n",
    "estimate_pipeline(df10_pipeline)\n",
    "\n",
    "print(\"Estimating pipeline with count vectorizer and min_df = 50:\")\n",
    "estimate_pipeline(df50_pipeline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estimating different classifiers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimating Logistic Regression\n",
      "CV mean score: 0.8415000000000001 Standard deviation: 0.01677796173556255\n",
      "Estimating SVM Classifier\n",
      "CV mean score: 0.8325000000000001 Standard deviation: 0.0162788205960997\n",
      "Estimating SGD Classifier\n",
      "CV mean score: 0.7505 Standard deviation: 0.06777536425575299\n"
     ]
    }
   ],
   "source": [
    "classifiers = {\"Logistic Regression\": LogisticRegression(),\n",
    "               \"SVM Classifier\": LinearSVC(),\n",
    "               \"SGD Classifier\": SGDClassifier()}\n",
    "\n",
    "for name, clf in classifiers.items():\n",
    "    pipeline = Pipeline(steps=[\n",
    "        ('vectorizer', CountVectorizer()),\n",
    "        ('estimator', clf)\n",
    "    ])\n",
    "    print(\"Estimating {}\".format(name))\n",
    "    estimate_pipeline(pipeline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For better model performance we should include stop-words. Lists of stop words are available both in sklearn and nltk packages.\n",
    "\n",
    "We will estimate model performance on both lists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we']\n"
     ]
    }
   ],
   "source": [
    "stop_words_nltk = nltk.corpus.stopwords.words('english')\n",
    "print(stop_words_nltk[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords_nltk_pipeline = Pipeline(steps=[\n",
    "        ('vectorizer', CountVectorizer(stop_words=stop_words_nltk)),\n",
    "        ('estimator', LogisticRegression())\n",
    "])\n",
    "\n",
    "stopwords_sklearn_pipeline = Pipeline(steps=[\n",
    "        ('vectorizer', CountVectorizer(stop_words='english')),\n",
    "        ('estimator', LogisticRegression())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimating pipeline with NLTK stopwords:\n",
      "CV mean score: 0.8414999999999999 Standard deviation: 0.010440306508910566\n",
      "Estimating pipeline with SKLearn stopwords list:\n",
      "CV mean score: 0.8385 Standard deviation: 0.009823441352194272\n"
     ]
    }
   ],
   "source": [
    "print(\"Estimating pipeline with NLTK stopwords:\")\n",
    "estimate_pipeline(stopwords_nltk_pipeline)\n",
    "\n",
    "print(\"Estimating pipeline with SKLearn stopwords list:\")\n",
    "estimate_pipeline(stopwords_sklearn_pipeline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we will estimate the performance of classifier with different N-grams ranges:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "ngram_words_pipeline = Pipeline(steps=[\n",
    "        ('vectorizer', CountVectorizer(ngram_range=(1, 2))),\n",
    "        ('estimator', LogisticRegression())\n",
    "])\n",
    "\n",
    "ngram_chars_pipeline = Pipeline(steps=[\n",
    "        ('vectorizer', CountVectorizer(ngram_range=(3, 5), analyzer='char_wb')),\n",
    "        ('estimator', LogisticRegression())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimating pipeline with word bigrams:\n",
      "CV mean score: 0.852 Standard deviation: 0.016537835408541222\n",
      "Estimating pipeline with 3-5 character n-grams:\n",
      "CV mean score: 0.82 Standard deviation: 0.010606601717798201\n"
     ]
    }
   ],
   "source": [
    "print(\"Estimating pipeline with word bigrams:\")\n",
    "estimate_pipeline(ngram_words_pipeline)\n",
    "\n",
    "print(\"Estimating pipeline with 3-5 character n-grams:\")\n",
    "estimate_pipeline(ngram_chars_pipeline)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
